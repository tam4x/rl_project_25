{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207c7fad",
   "metadata": {},
   "source": [
    "##### Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7af34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import Wrapper\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, Any, Tuple, List\n",
    "import os\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3 import PPO, SAC, TD3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85866f44",
   "metadata": {},
   "source": [
    "##### Task Variants for Cheetah (Target Velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417ce016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HalfCheetahTargetVelocity(Wrapper):\n",
    "    def __init__(self, env, target_velocity: float, vel_scale: float = 1.0, ctrl_cost_weight: float = 0.1):\n",
    "        super().__init__(env)\n",
    "        self.vt = float(target_velocity)\n",
    "        self.vel_scale = float(vel_scale)\n",
    "        self.ctrl_cost_weight = float(ctrl_cost_weight)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        vx = float(self.env.unwrapped.data.qvel[0])  # forward velocity\n",
    "        vel_reward = -abs(vx - self.vt) * self.vel_scale\n",
    "        ctrl_cost = self.ctrl_cost_weight * float(np.sum(action**2))\n",
    "        reward = vel_reward - ctrl_cost\n",
    "\n",
    "        info = dict(info)\n",
    "        info.update({\"vx\": vx, \"target_v\": self.vt})\n",
    "        return obs, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5b154",
   "metadata": {},
   "source": [
    "##### Task Variants for Ant (Target Direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntTargetDirection(Wrapper):\n",
    "    def __init__(self, env, direction: np.ndarray, ctrl_cost_weight: float = 0.05):\n",
    "        super().__init__(env)\n",
    "        d = np.asarray(direction, dtype=np.float32)\n",
    "        self.dir = d / (np.linalg.norm(d) + 1e-8)\n",
    "        self.ctrl_cost_weight = float(ctrl_cost_weight)\n",
    "        self._prev_xy = None\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self._prev_xy = np.array(self.env.unwrapped.data.qpos[0:2], dtype=np.float32)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        xy = np.array(self.env.unwrapped.data.qpos[0:2], dtype=np.float32)\n",
    "        dt = float(self.env.unwrapped.dt)\n",
    "        vel_xy = (xy - self._prev_xy) / max(dt, 1e-8)\n",
    "        self._prev_xy = xy\n",
    "\n",
    "        dir_speed = float(np.dot(vel_xy, self.dir))\n",
    "        ctrl_cost = self.ctrl_cost_weight * float(np.sum(action**2))\n",
    "        reward = dir_speed - ctrl_cost\n",
    "\n",
    "        info = dict(info)\n",
    "        info.update({\"vel_xy\": vel_xy, \"dir\": self.dir, \"dir_speed\": dir_speed})\n",
    "        return obs, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f1088b",
   "metadata": {},
   "source": [
    "##### Task Variants Walker (Target Height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01be4fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Walker2dTargetHeight(Wrapper):\n",
    "    def __init__(self, env, target_height: float, height_scale: float = 1.0, ctrl_cost_weight: float = 0.001):\n",
    "        super().__init__(env)\n",
    "        self.ht = float(target_height)\n",
    "        self.height_scale = float(height_scale)\n",
    "        self.ctrl_cost_weight = float(ctrl_cost_weight)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        height = float(self.env.unwrapped.data.qpos[1])  # torso height (typ.)\n",
    "        height_reward = -abs(height - self.ht) * self.height_scale\n",
    "        ctrl_cost = self.ctrl_cost_weight * float(np.sum(action**2))\n",
    "        reward = height_reward - ctrl_cost\n",
    "\n",
    "        info = dict(info)\n",
    "        info.update({\"height\": height, \"target_h\": self.ht})\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c46f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_base(env_id: str, seed: int = 0):\n",
    "    env = gym.make(env_id)\n",
    "    env.reset(seed=seed)\n",
    "    return env\n",
    "\n",
    "def task_halfcheetah_target_velocity(target_v: float, seed: int = 0):\n",
    "    env = gym.make(\"HalfCheetah-v4\")\n",
    "    env = HalfCheetahTargetVelocity(env, target_velocity=target_v)\n",
    "    env.reset(seed=seed)\n",
    "    return env\n",
    "\n",
    "def task_ant_target_direction(dx: float, dy: float, seed: int = 0):\n",
    "    env = gym.make(\"Ant-v4\")\n",
    "    env = AntTargetDirection(env, direction=np.array([dx, dy], dtype=np.float32))\n",
    "    env.reset(seed=seed)\n",
    "    return env\n",
    "\n",
    "def task_walker2d_target_height(target_h: float, seed: int = 0):\n",
    "    env = gym.make(\"Walker2d-v4\")\n",
    "    env = Walker2dTargetHeight(env, target_height=target_h)\n",
    "    env.reset(seed=seed)\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Task:\n",
    "    name: str\n",
    "    make_env: Callable[[], gym.Env]\n",
    "\n",
    "\n",
    "tasks: List[Task] = [\n",
    "    Task(\"BASE HalfCheetah\", lambda: task_base(\"HalfCheetah-v4\", seed=0)),\n",
    "    Task(\"BASE Hopper\",      lambda: task_base(\"Hopper-v4\", seed=0)),\n",
    "    Task(\"BASE Walker2d\",    lambda: task_base(\"Walker2d-v4\", seed=0)),\n",
    "    Task(\"BASE Ant\",         lambda: task_base(\"Ant-v4\", seed=0))\n",
    "\n",
    "    #Task(\"HC target_v=1.0\",  lambda: task_halfcheetah_target_velocity(1.0, seed=1)),\n",
    "    #Task(\"HC target_v=2.0\",  lambda: task_halfcheetah_target_velocity(2.0, seed=2)),\n",
    "\n",
    "    #Task(\"Ant EAST\",         lambda: task_ant_target_direction(1.0, 0.0, seed=3)),\n",
    "    #Task(\"Ant NORTH\",        lambda: task_ant_target_direction(0.0, 1.0, seed=4)),\n",
    "\n",
    "    #Task(\"Walker h=1.2\",     lambda: task_walker2d_target_height(1.2, seed=5)),\n",
    "    #Task(\"Walker h=1.6\",     lambda: task_walker2d_target_height(1.6, seed=6)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe005c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vec_env(task: Task, seed: int = 0, normalize_obs: bool = True):\n",
    "    def _init():\n",
    "        env = task.make_env()\n",
    "        env = Monitor(env)  # logs episode returns/lengths\n",
    "        env.reset(seed=seed)\n",
    "        return env\n",
    "\n",
    "    venv = DummyVecEnv([_init])\n",
    "\n",
    "    if normalize_obs:\n",
    "        venv = VecNormalize(venv, norm_obs=True, norm_reward=False, clip_obs=10.0)\n",
    "\n",
    "    return venv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57270f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_teacher(algo: str, env, seed: int = 0, logdir: str = None):\n",
    "    algo = algo.upper()\n",
    "    common = dict(verbose=1, seed=seed, tensorboard_log=logdir)\n",
    "\n",
    "    if algo == \"SAC\":\n",
    "        return SAC(\"MlpPolicy\", env, batch_size=256, learning_rate=3e-4, gamma=0.99, **common)\n",
    "\n",
    "    if algo == \"TD3\":\n",
    "        return TD3(\"MlpPolicy\", env, batch_size=256, learning_rate=1e-3, gamma=0.99, **common)\n",
    "\n",
    "    if algo == \"PPO\":\n",
    "        return PPO(\"MlpPolicy\", env, n_steps=2048, batch_size=64, learning_rate=3e-4, gamma=0.99, **common)\n",
    "\n",
    "    raise ValueError(f\"Unknown algo: {algo}\")\n",
    "\n",
    "\n",
    "def train_teacher_for_task(\n",
    "    task: Task,\n",
    "    algo: str = \"SAC\",\n",
    "    total_timesteps: int = 300_000,\n",
    "    seed: int = 0,\n",
    "    normalize_obs: bool = True,\n",
    "    out_dir: str = \"./teachers\",\n",
    "    log_dir: str = \"./tb_logs\",\n",
    "):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Build env\n",
    "    venv = build_vec_env(task, seed=seed, normalize_obs=normalize_obs)\n",
    "\n",
    "    # Train teacher\n",
    "    model = make_teacher(algo, venv, seed=seed, logdir=log_dir)\n",
    "    model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
    "\n",
    "    # Evaluate (freeze normalization updates)\n",
    "    venv.training = False\n",
    "    venv.norm_reward = False\n",
    "\n",
    "    mean_r, std_r = evaluate_policy(model, venv, n_eval_episodes=10, deterministic=True)\n",
    "    print(f\"[{task.name}] {algo} eval: {mean_r:.2f} +/- {std_r:.2f}\")\n",
    "\n",
    "    # Save\n",
    "    model_path = os.path.join(out_dir, f\"{task.name}_{algo}.zip\")\n",
    "    model.save(model_path)\n",
    "\n",
    "    vec_path = None\n",
    "    if isinstance(venv, VecNormalize):\n",
    "        vec_path = os.path.join(out_dir, f\"{task.name}_{algo}_vecnormalize.pkl\")\n",
    "        venv.save(vec_path)\n",
    "\n",
    "    venv.close()\n",
    "    return {\"task\": task.name, \"algo\": algo, \"mean\": mean_r, \"std\": std_r, \"model_path\": model_path, \"vec_path\": vec_path}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29848036",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i, task in enumerate(tasks):\n",
    "    res = train_teacher_for_task(\n",
    "        task=task,\n",
    "        algo=\"SAC\",\n",
    "        total_timesteps=1000000,\n",
    "        seed=100 + i,\n",
    "        normalize_obs=True,\n",
    "        out_dir=\"./teachers\",\n",
    "        log_dir=\"./tb_logs\",\n",
    "    )\n",
    "    results.append(res)\n",
    "\n",
    "## tensorboard --logdir tb_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa43abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf470ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_teacher(model_path: str, vec_path: str, task: Task, seed: int = 0):\n",
    "    # rebuild env (must match!)\n",
    "    venv = build_vec_env(task, seed=seed, normalize_obs=False)\n",
    "    if vec_path is not None:\n",
    "        venv = VecNormalize.load(vec_path, venv)\n",
    "        venv.training = False\n",
    "        venv.norm_reward = False\n",
    "\n",
    "    model = SAC.load(model_path, env=venv)  # if you used TD3/PPO, load with that class\n",
    "    return model, venv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_rom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
