{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzZhaL0zgUJ6",
        "outputId": "1ffe35d8-55f5-4f81-a015-f9dbc1255c8b"
      },
      "id": "PzZhaL0zgUJ6",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.9.0+cu126)\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.2.2)\n",
            "Collecting stable-baselines3[extra] (from -r requirements.txt (line 3))\n",
            "  Downloading stable_baselines3-2.7.1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]->-r requirements.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]->-r requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]->-r requirements.txt (line 2)) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco]->-r requirements.txt (line 2))\n",
            "  Downloading mujoco-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]->-r requirements.txt (line 2)) (2.37.2)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[mujoco]->-r requirements.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 3)) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 3)) (4.12.0.88)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 3)) (2.19.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 3)) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 3)) (0.11.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]->-r requirements.txt (line 3)) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.12/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]->-r requirements.txt (line 2)) (1.13.0)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco]->-r requirements.txt (line 2))\n",
            "  Downloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]->-r requirements.txt (line 2)) (3.1.10)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]->-r requirements.txt (line 3)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]->-r requirements.txt (line 3)) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]->-r requirements.txt (line 3)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]->-r requirements.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 4)) (3.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]->-r requirements.txt (line 3)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]->-r requirements.txt (line 3)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]->-r requirements.txt (line 3)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]->-r requirements.txt (line 3)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]->-r requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3[extra]->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3[extra]->-r requirements.txt (line 3)) (2025.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->stable-baselines3[extra]->-r requirements.txt (line 3)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->stable-baselines3[extra]->-r requirements.txt (line 3)) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]->-r requirements.txt (line 3)) (0.1.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]->-r requirements.txt (line 2)) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]->-r requirements.txt (line 2)) (3.23.0)\n",
            "Downloading mujoco-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.7.1-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.0/188.0 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco, stable-baselines3\n",
            "Successfully installed glfw-2.10.0 mujoco-3.4.0 stable-baselines3-2.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "207c7fad",
      "metadata": {
        "id": "207c7fad"
      },
      "source": [
        "##### Import Libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1c7af34a",
      "metadata": {
        "id": "1c7af34a",
        "outputId": "1bbe3937-eb73-4abf-a386-c00a71f2d4c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import Wrapper\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable, Dict, Any, Tuple, List\n",
        "import os\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3 import PPO, SAC, TD3\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85866f44",
      "metadata": {
        "id": "85866f44"
      },
      "source": [
        "##### Task Variants for Cheetah (Target Velocity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "417ce016",
      "metadata": {
        "id": "417ce016"
      },
      "outputs": [],
      "source": [
        "class HalfCheetahTargetVelocity(Wrapper):\n",
        "    def __init__(self, env, target_velocity: float, vel_scale: float = 1.0, ctrl_cost_weight: float = 0.1):\n",
        "        super().__init__(env)\n",
        "        self.vt = float(target_velocity)\n",
        "        self.vel_scale = float(vel_scale)\n",
        "        self.ctrl_cost_weight = float(ctrl_cost_weight)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, _, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        vx = float(self.env.unwrapped.data.qvel[0])  # forward velocity\n",
        "        vel_reward = -abs(vx - self.vt) * self.vel_scale\n",
        "        ctrl_cost = self.ctrl_cost_weight * float(np.sum(action**2))\n",
        "        reward = vel_reward - ctrl_cost\n",
        "\n",
        "        info = dict(info)\n",
        "        info.update({\"vx\": vx, \"target_v\": self.vt})\n",
        "        return obs, reward, terminated, truncated, info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16a5b154",
      "metadata": {
        "id": "16a5b154"
      },
      "source": [
        "##### Task Variants for Ant (Target Direction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "03e0929e",
      "metadata": {
        "id": "03e0929e"
      },
      "outputs": [],
      "source": [
        "class AntTargetDirection(Wrapper):\n",
        "    def __init__(self, env, direction: np.ndarray, ctrl_cost_weight: float = 0.05):\n",
        "        super().__init__(env)\n",
        "        d = np.asarray(direction, dtype=np.float32)\n",
        "        self.dir = d / (np.linalg.norm(d) + 1e-8)\n",
        "        self.ctrl_cost_weight = float(ctrl_cost_weight)\n",
        "        self._prev_xy = None\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self._prev_xy = np.array(self.env.unwrapped.data.qpos[0:2], dtype=np.float32)\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, _, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        xy = np.array(self.env.unwrapped.data.qpos[0:2], dtype=np.float32)\n",
        "        dt = float(self.env.unwrapped.dt)\n",
        "        vel_xy = (xy - self._prev_xy) / max(dt, 1e-8)\n",
        "        self._prev_xy = xy\n",
        "\n",
        "        dir_speed = float(np.dot(vel_xy, self.dir))\n",
        "        ctrl_cost = self.ctrl_cost_weight * float(np.sum(action**2))\n",
        "        reward = dir_speed - ctrl_cost\n",
        "\n",
        "        info = dict(info)\n",
        "        info.update({\"vel_xy\": vel_xy, \"dir\": self.dir, \"dir_speed\": dir_speed})\n",
        "        return obs, reward, terminated, truncated, info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59f1088b",
      "metadata": {
        "id": "59f1088b"
      },
      "source": [
        "##### Task Variants Walker (Target Height)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "01be4fd9",
      "metadata": {
        "id": "01be4fd9"
      },
      "outputs": [],
      "source": [
        "class Walker2dTargetHeight(Wrapper):\n",
        "    def __init__(self, env, target_height: float, height_scale: float = 1.0, ctrl_cost_weight: float = 0.001):\n",
        "        super().__init__(env)\n",
        "        self.ht = float(target_height)\n",
        "        self.height_scale = float(height_scale)\n",
        "        self.ctrl_cost_weight = float(ctrl_cost_weight)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, _, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        height = float(self.env.unwrapped.data.qpos[1])  # torso height (typ.)\n",
        "        height_reward = -abs(height - self.ht) * self.height_scale\n",
        "        ctrl_cost = self.ctrl_cost_weight * float(np.sum(action**2))\n",
        "        reward = height_reward - ctrl_cost\n",
        "\n",
        "        info = dict(info)\n",
        "        info.update({\"height\": height, \"target_h\": self.ht})\n",
        "        return obs, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e4c46f28",
      "metadata": {
        "id": "e4c46f28"
      },
      "outputs": [],
      "source": [
        "def task_base(env_id: str, seed: int = 0):\n",
        "    env = gym.make(env_id)\n",
        "    env.reset(seed=seed)\n",
        "    return env\n",
        "\n",
        "def task_halfcheetah_target_velocity(target_v: float, seed: int = 0):\n",
        "    env = gym.make(\"HalfCheetah-v4\")\n",
        "    env = HalfCheetahTargetVelocity(env, target_velocity=target_v)\n",
        "    env.reset(seed=seed)\n",
        "    return env\n",
        "\n",
        "def task_ant_target_direction(dx: float, dy: float, seed: int = 0):\n",
        "    env = gym.make(\"Ant-v4\")\n",
        "    env = AntTargetDirection(env, direction=np.array([dx, dy], dtype=np.float32))\n",
        "    env.reset(seed=seed)\n",
        "    return env\n",
        "\n",
        "def task_walker2d_target_height(target_h: float, seed: int = 0):\n",
        "    env = gym.make(\"Walker2d-v4\")\n",
        "    env = Walker2dTargetHeight(env, target_height=target_h)\n",
        "    env.reset(seed=seed)\n",
        "    return env\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "51aa01f2",
      "metadata": {
        "id": "51aa01f2"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class Task:\n",
        "    name: str\n",
        "    make_env: Callable[[], gym.Env]\n",
        "\n",
        "\n",
        "tasks: List[Task] = [\n",
        "    Task(\"BASE HalfCheetah\", lambda: task_base(\"HalfCheetah-v4\", seed=0)),\n",
        "    Task(\"BASE Hopper\",      lambda: task_base(\"Hopper-v4\", seed=0)),\n",
        "    Task(\"BASE Walker2d\",    lambda: task_base(\"Walker2d-v4\", seed=0)),\n",
        "    Task(\"BASE Ant\",         lambda: task_base(\"Ant-v4\", seed=0))\n",
        "\n",
        "    #Task(\"HC target_v=1.0\",  lambda: task_halfcheetah_target_velocity(1.0, seed=1)),\n",
        "    #Task(\"HC target_v=2.0\",  lambda: task_halfcheetah_target_velocity(2.0, seed=2)),\n",
        "\n",
        "    #Task(\"Ant EAST\",         lambda: task_ant_target_direction(1.0, 0.0, seed=3)),\n",
        "    #Task(\"Ant NORTH\",        lambda: task_ant_target_direction(0.0, 1.0, seed=4)),\n",
        "\n",
        "    #Task(\"Walker h=1.2\",     lambda: task_walker2d_target_height(1.2, seed=5)),\n",
        "    #Task(\"Walker h=1.6\",     lambda: task_walker2d_target_height(1.6, seed=6)),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fe005c31",
      "metadata": {
        "id": "fe005c31"
      },
      "outputs": [],
      "source": [
        "def build_vec_env(task: Task, seed: int = 0, normalize_obs: bool = True):\n",
        "    def _init():\n",
        "        env = task.make_env()\n",
        "        env = Monitor(env)  # logs episode returns/lengths\n",
        "        env.reset(seed=seed)\n",
        "        return env\n",
        "\n",
        "    venv = DummyVecEnv([_init])\n",
        "\n",
        "    if normalize_obs:\n",
        "        venv = VecNormalize(venv, norm_obs=True, norm_reward=False, clip_obs=10.0)\n",
        "\n",
        "    return venv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "57270f28",
      "metadata": {
        "id": "57270f28"
      },
      "outputs": [],
      "source": [
        "def make_teacher(algo: str, env, seed: int = 0, logdir: str = None):\n",
        "    algo = algo.upper()\n",
        "    common = dict(verbose=1, seed=seed, tensorboard_log=logdir)\n",
        "\n",
        "    if algo == \"SAC\":\n",
        "        return SAC(\"MlpPolicy\", env, batch_size=256, learning_rate=3e-4, gamma=0.99, **common)\n",
        "\n",
        "    if algo == \"TD3\":\n",
        "        return TD3(\"MlpPolicy\", env, batch_size=256, learning_rate=1e-3, gamma=0.99, **common)\n",
        "\n",
        "    if algo == \"PPO\":\n",
        "        return PPO(\"MlpPolicy\", env, n_steps=2048, batch_size=64, learning_rate=3e-4, gamma=0.99, **common)\n",
        "\n",
        "    raise ValueError(f\"Unknown algo: {algo}\")\n",
        "\n",
        "\n",
        "def train_teacher_for_task(\n",
        "    task: Task,\n",
        "    algo: str = \"SAC\",\n",
        "    total_timesteps: int = 300_000,\n",
        "    seed: int = 0,\n",
        "    normalize_obs: bool = True,\n",
        "    out_dir: str = \"./teachers\",\n",
        "    log_dir: str = \"./tb_logs\",\n",
        "):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    # Build env\n",
        "    venv = build_vec_env(task, seed=seed, normalize_obs=normalize_obs)\n",
        "\n",
        "    # Train teacher\n",
        "    model = make_teacher(algo, venv, seed=seed, logdir=log_dir)\n",
        "    model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
        "\n",
        "    # Evaluate (freeze normalization updates)\n",
        "    venv.training = False\n",
        "    venv.norm_reward = False\n",
        "\n",
        "    mean_r, std_r = evaluate_policy(model, venv, n_eval_episodes=10, deterministic=True)\n",
        "    print(f\"[{task.name}] {algo} eval: {mean_r:.2f} +/- {std_r:.2f}\")\n",
        "\n",
        "    # Save\n",
        "    model_path = os.path.join(out_dir, f\"{task.name}_{algo}.zip\")\n",
        "    model.save(model_path)\n",
        "\n",
        "    vec_path = None\n",
        "    if isinstance(venv, VecNormalize):\n",
        "        vec_path = os.path.join(out_dir, f\"{task.name}_{algo}_vecnormalize.pkl\")\n",
        "        venv.save(vec_path)\n",
        "\n",
        "    venv.close()\n",
        "    return {\"task\": task.name, \"algo\": algo, \"mean\": mean_r, \"std\": std_r, \"model_path\": model_path, \"vec_path\": vec_path}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "29848036",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "29848036",
        "outputId": "cbf1b7cb-2d76-4433-f707-7d1f9a5fcb75"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[35m 100%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m499,993/500,000 \u001b[0m [ \u001b[33m1:56:42\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m73 it/s\u001b[0m ]\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 100%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">499,993/500,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">1:56:42</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">73 it/s</span> ]\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASE Ant] SAC eval: 3602.85 +/- 70.70\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "for i, task in enumerate(tasks):\n",
        "    res = train_teacher_for_task(\n",
        "        task=task,\n",
        "        algo=\"SAC\",\n",
        "        total_timesteps=500000,\n",
        "        seed=100 + i,\n",
        "        normalize_obs=True,\n",
        "        out_dir=\"./teachers\",\n",
        "        log_dir=\"./tb_logs\",\n",
        "    )\n",
        "    results.append(res)\n",
        "\n",
        "## tensorboard --logdir tb_logs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5aa43abe",
      "metadata": {
        "id": "5aa43abe",
        "outputId": "d79403b0-766a-4dfc-d028-bad3286bf641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'task': 'BASE HalfCheetah',\n",
              "  'algo': 'SAC',\n",
              "  'mean': np.float64(8743.4280451),\n",
              "  'std': np.float64(122.44090484187974),\n",
              "  'model_path': './teachers/BASE HalfCheetah_SAC.zip',\n",
              "  'vec_path': './teachers/BASE HalfCheetah_SAC_vecnormalize.pkl'},\n",
              " {'task': 'BASE Hopper',\n",
              "  'algo': 'SAC',\n",
              "  'mean': np.float64(3534.9982952),\n",
              "  'std': np.float64(74.30036539246545),\n",
              "  'model_path': './teachers/BASE Hopper_SAC.zip',\n",
              "  'vec_path': './teachers/BASE Hopper_SAC_vecnormalize.pkl'},\n",
              " {'task': 'BASE Walker2d',\n",
              "  'algo': 'SAC',\n",
              "  'mean': np.float64(4432.8367175),\n",
              "  'std': np.float64(88.3051954926245),\n",
              "  'model_path': './teachers/BASE Walker2d_SAC.zip',\n",
              "  'vec_path': './teachers/BASE Walker2d_SAC_vecnormalize.pkl'},\n",
              " {'task': 'BASE Ant',\n",
              "  'algo': 'SAC',\n",
              "  'mean': np.float64(3602.8521288),\n",
              "  'std': np.float64(70.70085649305115),\n",
              "  'model_path': './teachers/BASE Ant_SAC.zip',\n",
              "  'vec_path': './teachers/BASE Ant_SAC_vecnormalize.pkl'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Load Teacher for Memory Creation"
      ],
      "metadata": {
        "id": "3jOGJeXGp5EQ"
      },
      "id": "3jOGJeXGp5EQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf470ed9",
      "metadata": {
        "id": "bf470ed9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Callable, Optional, Dict, Any, List\n",
        "\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "\n",
        "def load_sac_teacher(task: Task, model_path: str, vec_path: Optional[str], seed: int = 0):\n",
        "    \"\"\"\n",
        "    Loads SAC model + VecNormalize stats (if provided) for correct obs normalization.\n",
        "    Returns (model, venv) where venv is ready for inference/eval.\n",
        "    \"\"\"\n",
        "    venv = build_vec_env(task, seed=seed, normalize_obs=False)\n",
        "\n",
        "    if vec_path is not None:\n",
        "        venv = VecNormalize.load(vec_path, venv)\n",
        "        venv.training = False\n",
        "        venv.norm_reward = False\n",
        "\n",
        "    model = SAC.load(model_path, env=venv)\n",
        "    return model, venv\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def sac_policy_params(model: SAC, obs_batch: np.ndarray):\n",
        "    \"\"\"\n",
        "    obs_batch: (n_envs, obs_dim) in VecEnv format (here n_envs=1).\n",
        "    Returns:\n",
        "      mu:      (n_envs, act_dim)\n",
        "      log_std: (n_envs, act_dim)\n",
        "    \"\"\"\n",
        "    obs_t = torch.as_tensor(obs_batch).to(model.device)\n",
        "\n",
        "    # SB3 SAC actor helper\n",
        "    mu_t, log_std_t, _ = model.policy.actor.get_action_dist_params(obs_t)\n",
        "\n",
        "    mu = mu_t.detach().cpu().numpy()\n",
        "    log_std = log_std_t.detach().cpu().numpy()\n",
        "    return mu, log_std\n"
      ],
      "metadata": {
        "id": "N6GYKiqjqg-g"
      },
      "id": "N6GYKiqjqg-g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_memory_from_sac_teacher(\n",
        "    model: SAC,\n",
        "    venv,\n",
        "    task_name: str,\n",
        "    n_steps: int = 100_000,\n",
        "    deterministic_action: bool = True,\n",
        "    store_actions: bool = True,\n",
        "    seed: int = 0,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Collects memory dataset: obs_norm, mu, log_std, (optional) action.\n",
        "    NOTE: obs from VecNormalize-wrapped venv are already normalized.\n",
        "    \"\"\"\n",
        "    venv.seed(seed)\n",
        "    obs = venv.reset()\n",
        "\n",
        "    obs_list = []\n",
        "    mu_list = []\n",
        "    logstd_list = []\n",
        "    act_list = []\n",
        "\n",
        "    for _ in range(n_steps):\n",
        "        mu, log_std = sac_policy_params(model, obs)\n",
        "\n",
        "        action, _ = model.predict(obs, deterministic=deterministic_action)\n",
        "\n",
        "        obs_list.append(obs.copy())\n",
        "        mu_list.append(mu.copy())\n",
        "        logstd_list.append(log_std.copy())\n",
        "        if store_actions:\n",
        "            act_list.append(action.copy())\n",
        "\n",
        "        obs, reward, done, info = venv.step(action)\n",
        "\n",
        "        if bool(done[0]):\n",
        "            obs = venv.reset()\n",
        "\n",
        "    data = {\n",
        "        \"task\": task_name,\n",
        "        \"obs\": np.concatenate(obs_list, axis=0),        # (n_steps, obs_dim)\n",
        "        \"mu\": np.concatenate(mu_list, axis=0),          # (n_steps, act_dim)\n",
        "        \"log_std\": np.concatenate(logstd_list, axis=0), # (n_steps, act_dim)\n",
        "    }\n",
        "    if store_actions:\n",
        "        data[\"action\"] = np.concatenate(act_list, axis=0)\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "BQ1T88Q-qiIZ"
      },
      "id": "BQ1T88Q-qiIZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_memory_npz(data: Dict[str, Any], out_path: str):\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    np.savez_compressed(\n",
        "        out_path,\n",
        "        task=data[\"task\"],\n",
        "        obs=data[\"obs\"],\n",
        "        mu=data[\"mu\"],\n",
        "        log_std=data[\"log_std\"],\n",
        "        **({\"action\": data[\"action\"]} if \"action\" in data else {}),\n",
        "    )\n",
        "    print(\"Saved memory:\", out_path)\n"
      ],
      "metadata": {
        "id": "V3uapFV5sLhI"
      },
      "id": "V3uapFV5sLhI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MEM_DIR = \"./memory_sac\"\n",
        "all_mem_paths = []\n",
        "\n",
        "for i, t in enumerate(tasks):\n",
        "\n",
        "    r = results[i]\n",
        "\n",
        "    model, venv = load_sac_teacher(t, r[\"model_path\"], r[\"vec_path\"], seed=0)\n",
        "\n",
        "    mem = collect_memory_from_sac_teacher(\n",
        "        model=model,\n",
        "        venv=venv,\n",
        "        task_name=t.name,\n",
        "        n_steps=50_000,                # start small to validate\n",
        "        deterministic_action=True,     # or False to cover more state space\n",
        "        store_actions=True,\n",
        "        seed=123\n",
        "    )\n",
        "\n",
        "    out_path = os.path.join(MEM_DIR, f\"{t.name}_SAC_memory.npz\")\n",
        "    save_memory_npz(mem, out_path)\n",
        "    all_mem_paths.append(out_path)\n",
        "\n",
        "    venv.close()\n",
        "\n",
        "all_mem_paths\n"
      ],
      "metadata": {
        "id": "lWyFoytksMXB"
      },
      "id": "lWyFoytksMXB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "uX-bs1z-vuCR"
      },
      "id": "uX-bs1z-vuCR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistillMemoryDataset(Dataset):\n",
        "    def __init__(self, npz_path: str):\n",
        "        d = np.load(npz_path, allow_pickle=True)\n",
        "        self.obs = d[\"obs\"].astype(np.float32)\n",
        "        self.mu_t = d[\"mu\"].astype(np.float32)\n",
        "        self.log_std_t = d[\"log_std\"].astype(np.float32)\n",
        "        self.action_t = d[\"action\"].astype(np.float32) if \"action\" in d.files else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.obs.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        obs = self.obs[idx]\n",
        "        mu_t = self.mu_t[idx]\n",
        "        log_std_t = self.log_std_t[idx]\n",
        "        if self.action_t is None:\n",
        "            return obs, mu_t, log_std_t\n",
        "        return obs, mu_t, log_std_t, self.action_t[idx]\n"
      ],
      "metadata": {
        "id": "N6QXvmXJvvGJ"
      },
      "id": "N6QXvmXJvvGJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianStudentPolicy(nn.Module):\n",
        "    def __init__(self, obs_dim: int, act_dim: int, hidden=(256, 256), log_std_bounds=(-5.0, 2.0)):\n",
        "        super().__init__()\n",
        "        self.log_std_min, self.log_std_max = log_std_bounds\n",
        "\n",
        "        layers = []\n",
        "        in_dim = obs_dim\n",
        "        for h in hidden:\n",
        "            layers += [nn.Linear(in_dim, h), nn.ReLU()]\n",
        "            in_dim = h\n",
        "        self.backbone = nn.Sequential(*layers)\n",
        "\n",
        "        self.mu_head = nn.Linear(in_dim, act_dim)\n",
        "        self.log_std_head = nn.Linear(in_dim, act_dim)\n",
        "\n",
        "    def forward(self, obs: torch.Tensor):\n",
        "        x = self.backbone(obs)\n",
        "        mu = self.mu_head(x)\n",
        "        log_std = self.log_std_head(x)\n",
        "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
        "        return mu, log_std\n"
      ],
      "metadata": {
        "id": "8D32MzIqw4vO"
      },
      "id": "8D32MzIqw4vO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Distillation Method 1 and 2 use soft and hard label actions"
      ],
      "metadata": {
        "id": "4ezIQ-GJyxc-"
      },
      "id": "4ezIQ-GJyxc-"
    },
    {
      "cell_type": "code",
      "source": [
        "# D1\n",
        "def diag_gaussian_kl(mu_t, log_std_t, mu_s, log_std_s):\n",
        "    # shapes: (B, act_dim)\n",
        "    std_t = torch.exp(log_std_t)\n",
        "    std_s = torch.exp(log_std_s)\n",
        "\n",
        "    var_t = std_t ** 2\n",
        "    var_s = std_s ** 2\n",
        "\n",
        "    kl = (log_std_s - log_std_t) + (var_t + (mu_t - mu_s) ** 2) / (2.0 * var_s) - 0.5\n",
        "    return kl.sum(dim=-1).mean()  # mean over batch\n"
      ],
      "metadata": {
        "id": "QvrLdOGPw-yn"
      },
      "id": "QvrLdOGPw-yn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# D2\n",
        "def action_mse(mu_s, action_t):\n",
        "    return F.mse_loss(mu_s, action_t)"
      ],
      "metadata": {
        "id": "CKAURSftxDGE"
      },
      "id": "CKAURSftxDGE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Distillation Method 3 uses weighted certainty. States where the teacher is sure what to do are weighted harder"
      ],
      "metadata": {
        "id": "6gEQHUeYylpK"
      },
      "id": "6gEQHUeYylpK"
    },
    {
      "cell_type": "code",
      "source": [
        "# D3\n",
        "def certainty_weights(log_std_t, eps=1e-6):\n",
        "    # weight per sample (B,)\n",
        "    std_t = torch.exp(log_std_t)              # (B, act_dim)\n",
        "    w = 1.0 / (eps + std_t.mean(dim=-1))      # (B,)\n",
        "    # normalize weights to keep scale stable\n",
        "    w = w / (w.mean() + 1e-8)\n",
        "    return w\n",
        "\n",
        "def weighted_diag_gaussian_kl(mu_t, log_std_t, mu_s, log_std_s):\n",
        "    std_t = torch.exp(log_std_t)\n",
        "    std_s = torch.exp(log_std_s)\n",
        "    var_t = std_t ** 2\n",
        "    var_s = std_s ** 2\n",
        "\n",
        "    kl_per_dim = (log_std_s - log_std_t) + (var_t + (mu_t - mu_s) ** 2) / (2.0 * var_s) - 0.5\n",
        "    kl_per_sample = kl_per_dim.sum(dim=-1)  # (B,)\n",
        "\n",
        "    w = certainty_weights(log_std_t)         # (B,)\n",
        "    return (w * kl_per_sample).mean()\n"
      ],
      "metadata": {
        "id": "eS_1zyApxI8E"
      },
      "id": "eS_1zyApxI8E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_offline_distill(\n",
        "    npz_path: str,\n",
        "    method: str,\n",
        "    epochs: int = 10,\n",
        "    batch_size: int = 256,\n",
        "    lr: float = 3e-4,\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "):\n",
        "    ds = DistillMemoryDataset(npz_path)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    obs_dim = ds.obs.shape[1]\n",
        "    act_dim = ds.mu_t.shape[1]\n",
        "    student = GaussianStudentPolicy(obs_dim, act_dim).to(device)\n",
        "    opt = torch.optim.Adam(student.parameters(), lr=lr)\n",
        "\n",
        "    method = method.upper()\n",
        "    for ep in range(1, epochs + 1):\n",
        "        losses = []\n",
        "        for batch in dl:\n",
        "            opt.zero_grad()\n",
        "\n",
        "            if len(batch) == 3:\n",
        "                obs, mu_t, log_std_t = batch\n",
        "                action_t = None\n",
        "            else:\n",
        "                obs, mu_t, log_std_t, action_t = batch\n",
        "\n",
        "            obs = obs.to(device)\n",
        "            mu_t = mu_t.to(device)\n",
        "            log_std_t = log_std_t.to(device)\n",
        "            if action_t is not None:\n",
        "                action_t = action_t.to(device)\n",
        "\n",
        "            mu_s, log_std_s = student(obs)\n",
        "\n",
        "            if method == \"D1_KL\":\n",
        "                loss = diag_gaussian_kl(mu_t, log_std_t, mu_s, log_std_s)\n",
        "\n",
        "            elif method == \"D2_MSE\":\n",
        "                if action_t is None:\n",
        "                    raise ValueError(\"D2_MSE needs 'action' stored in npz.\")\n",
        "                loss = action_mse(mu_s, action_t)\n",
        "\n",
        "            elif method == \"D3_WKL\":\n",
        "                loss = weighted_diag_gaussian_kl(mu_t, log_std_t, mu_s, log_std_s)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Unknown method. Use: D1_KL, D2_MSE, D3_WKL\")\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        print(f\"Epoch {ep:02d} | {method} loss: {np.mean(losses):.4f}\")\n",
        "\n",
        "    return student\n",
        "\n"
      ],
      "metadata": {
        "id": "hls4okslxrkM"
      },
      "id": "hls4okslxrkM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_student(student, path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save(student.state_dict(), path)\n",
        "    print(\"Saved student:\", path)\n"
      ],
      "metadata": {
        "id": "QYU_pp7Yx31q"
      },
      "id": "QYU_pp7Yx31q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Distillation Method 4 uses normal RL for the student, but guides it through the teacher memory"
      ],
      "metadata": {
        "id": "DSqVfhV1yaTD"
      },
      "id": "DSqVfhV1yaTD"
    },
    {
      "cell_type": "code",
      "source": [
        "def diag_gaussian_kl_torch(mu_t, log_std_t, mu_s, log_std_s):\n",
        "    std_t = torch.exp(log_std_t); std_s = torch.exp(log_std_s)\n",
        "    var_t = std_t**2; var_s = std_s**2\n",
        "    kl = (log_std_s - log_std_t) + (var_t + (mu_t - mu_s)**2) / (2.0 * var_s) - 0.5\n",
        "    return kl.sum(dim=-1).mean()\n",
        "\n",
        "def sac_actor_distill_step(student_sac, obs_np, mu_teacher_np, log_std_teacher_np):\n",
        "    device = student_sac.device\n",
        "    actor = student_sac.policy.actor\n",
        "    opt = actor.optimizer\n",
        "\n",
        "    obs = torch.as_tensor(obs_np, dtype=torch.float32, device=device)\n",
        "    mu_t = torch.as_tensor(mu_teacher_np, dtype=torch.float32, device=device)\n",
        "    ls_t = torch.as_tensor(log_std_teacher_np, dtype=torch.float32, device=device)\n",
        "\n",
        "    mu_s, ls_s, _ = actor.get_action_dist_params(obs)\n",
        "    loss = diag_gaussian_kl_torch(mu_t, ls_t, mu_s, ls_s)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    return float(loss.item())\n"
      ],
      "metadata": {
        "id": "U3ncWKQiyZas"
      },
      "id": "U3ncWKQiyZas",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_kickstarting(\n",
        "    student_sac,\n",
        "    memory_npz_path: str,\n",
        "    total_timesteps: int = 300_000,\n",
        "    chunk: int = 20_000,\n",
        "    distill_steps_per_chunk: int = 200,\n",
        "    distill_batch_size: int = 256,\n",
        "):\n",
        "    mem = np.load(memory_npz_path)\n",
        "    obs_mem = mem[\"obs\"].astype(np.float32)\n",
        "    mu_mem  = mem[\"mu\"].astype(np.float32)\n",
        "    ls_mem  = mem[\"log_std\"].astype(np.float32)\n",
        "    n = obs_mem.shape[0]\n",
        "\n",
        "    trained = 0\n",
        "    while trained < total_timesteps:\n",
        "        student_sac.learn(total_timesteps=chunk, reset_num_timesteps=False, progress_bar=True)\n",
        "        trained += chunk\n",
        "\n",
        "        losses = []\n",
        "        for _ in range(distill_steps_per_chunk):\n",
        "            idx = np.random.randint(0, n, size=(distill_batch_size,))\n",
        "            loss = sac_actor_distill_step(student_sac, obs_mem[idx], mu_mem[idx], ls_mem[idx])\n",
        "            losses.append(loss)\n",
        "\n",
        "        print(f\"After {trained} steps | distill loss mean: {np.mean(losses):.4f}\")\n"
      ],
      "metadata": {
        "id": "_lPBqepPzWid"
      },
      "id": "_lPBqepPzWid",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Eval in a Normalized Env"
      ],
      "metadata": {
        "id": "sF5aBtHJ1Imf"
      },
      "id": "sF5aBtHJ1Imf"
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def make_base_vec_env(env_id: str, seed: int = 0):\n",
        "    def _init():\n",
        "        env = gym.make(env_id)\n",
        "        env = Monitor(env)\n",
        "        env.reset(seed=seed)\n",
        "        return env\n",
        "    return DummyVecEnv([_init])\n",
        "\n",
        "def load_eval_env_with_vecnorm(env_id: str, vec_path: str, seed: int = 0):\n",
        "    venv = make_base_vec_env(env_id, seed=seed)\n",
        "    venv = VecNormalize.load(vec_path, venv)\n",
        "    venv.training = False\n",
        "    venv.norm_reward = False\n",
        "    return venv\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_offline_student(student, venv, n_episodes=10, device=None):\n",
        "    if device is None:\n",
        "        device = next(student.parameters()).device\n",
        "    student.eval()\n",
        "\n",
        "    rets = []\n",
        "    for _ in range(n_episodes):\n",
        "        obs = venv.reset()        # normalized obs (shape (1, obs_dim))\n",
        "        done = [False]\n",
        "        ep_ret = 0.0\n",
        "\n",
        "        while not done[0]:\n",
        "            obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device)\n",
        "            mu, log_std = student(obs_t)\n",
        "\n",
        "            # MuJoCo expects actions in [-1, 1]; match SAC-style squashing:\n",
        "            action = torch.tanh(mu).cpu().numpy()\n",
        "\n",
        "            obs, reward, done, info = venv.step(action)\n",
        "            ep_ret += float(reward[0])\n",
        "\n",
        "        rets.append(ep_ret)\n",
        "\n",
        "    return float(np.mean(rets)), float(np.std(rets))\n"
      ],
      "metadata": {
        "id": "ehPhkCjO1H5v"
      },
      "id": "ehPhkCjO1H5v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training Run for Distillation"
      ],
      "metadata": {
        "id": "vj7Udo_m3cFS"
      },
      "id": "vj7Udo_m3cFS"
    },
    {
      "cell_type": "code",
      "source": [
        "npz_path = \"./memory_sac/BASE_HalfCheetah_SAC_memory.npz\"\n",
        "env_id = \"HalfCheetah-v4\"\n",
        "vec_path = \"./teachers/BASE_HalfCheetah_SAC_vecnormalize.pkl\"\n",
        "\n",
        "student_d1 = train_offline_distill(npz_path, \"D1_KL\", epochs=10)\n",
        "student_d2 = train_offline_distill(npz_path, \"D2_MSE\", epochs=10)\n",
        "student_d3 = train_offline_distill(npz_path, \"D3_WKL\", epochs=10)\n",
        "\n",
        "venv_eval = load_eval_env_with_vecnorm(env_id, vec_path, seed=0)\n",
        "\n",
        "print(\"D1:\", eval_offline_student(student_d1, venv_eval))\n",
        "print(\"D2:\", eval_offline_student(student_d2, venv_eval))\n",
        "print(\"D3:\", eval_offline_student(student_d3, venv_eval))\n",
        "\n",
        "venv_eval.close()\n"
      ],
      "metadata": {
        "id": "DIef_MFU3eSD"
      },
      "id": "DIef_MFU3eSD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import SAC\n",
        "\n",
        "venv_student = load_eval_env_with_vecnorm(env_id, vec_path, seed=0)  # normalized env\n",
        "\n",
        "student_sac = SAC(\"MlpPolicy\", venv_student, verbose=1, seed=0, batch_size=256, learning_rate=3e-4)\n",
        "train_kickstarting(\n",
        "    student_sac,\n",
        "    memory_npz_path=npz_path,\n",
        "    total_timesteps=300_000,\n",
        "    chunk=20_000,\n",
        "    distill_steps_per_chunk=200,\n",
        "    distill_batch_size=256,\n",
        ")\n",
        "\n",
        "rewards = evaluate_policy(student_sac, venv_student)"
      ],
      "metadata": {
        "id": "rPG2Lisk4HNU"
      },
      "id": "rPG2Lisk4HNU",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}